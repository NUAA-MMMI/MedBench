alg_name: "LoRA"
model_name: "./hugging_cache/qwen2-vl-7b"
device: 0

# LoRA settings
lora_type: "adalora"
layers: []
kl_factor: 0
norm_constraint: false
rank: 4
lora_alpha: 32
lora_dropout: 0.1
# target_modules: ["q_proj", "v_proj"]
target_modules: ["model.layers.*.self_attn.q_proj","model.layers.*.self_attn.v_proj"]
# Training settings
num_steps: 10
batch_size: 1
max_length: 30
lr: 0.01
sh_lr: 0.9
weight_decay: 0.0

# Additional settings
model_parallel: false

use_chat_template: True