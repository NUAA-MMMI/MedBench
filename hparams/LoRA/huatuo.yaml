alg_name: "LoRA"
model_name: "./hugging_cache/HuatuoGPT-Vision-7B-Qwen2.5VL"
device: 1

# LoRA settings
lora_type: "adalora"
layers: []
kl_factor: 0
norm_constraint: false
rank: 4
lora_alpha: 32
lora_dropout: 0.1
# target_modules: ["q_proj", "v_proj"]
target_modules: ["model.layers.*.self_attn.q_proj","model.layers.*.self_attn.v_proj"]
# Training settings
num_steps: 10
batch_size: 1
max_length: 30
lr: 0.1
sh_lr: 0.9
# lr: 1e-8
weight_decay: 0.0

# Additional settings
model_parallel: True

use_chat_template: True