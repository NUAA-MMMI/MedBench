alg_name: "LoRA"
model_name: "./hugging_cache/llava-onevision-qwen2-7b-ov-hf"
device: 1

# LoRA settings
lora_type: "adalora"
layers: []
#[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]
kl_factor: 0
norm_constraint: false
rank: 4
lora_alpha: 32
lora_dropout: 0.1
# target_modules: ["q_proj", "v_proj"]
target_modules: ["language_model.model.layers.*q_proj.*","language_model.model.layers.*v_proj.*"]

# Training settings
num_steps: 10
batch_size: 1
max_length: 30
lr: 0.01
sh_lr: 0.9
# lr: 1e-8
weight_decay: 0.0

# Additional settings
model_parallel: True

use_chat_template: True