[
  {
    "method_name": "Prompt_llavaov",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 5.0709922313690186,
    "editor_gpu_memory": 15927.0,
    "edit_execution_time": 29.33884835243225,
    "edit_gpu_memory": 3458.0,
    "total_time": 34.40984058380127,
    "peak_gpu_memory": 19385.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.8,
    "avg_time_per_sample": 5.86776967048645,
    "timestamp": "2025-07-25T13:18:31.959566"
  },
  {
    "method_name": "Prompt_qwen2vl",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.90581202507019,
    "editor_gpu_memory": 16100.0,
    "edit_execution_time": 7.947126388549805,
    "edit_gpu_memory": 780.0,
    "total_time": 12.852938413619995,
    "peak_gpu_memory": 17427.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.6,
    "avg_time_per_sample": 1.589425277709961,
    "timestamp": "2025-07-25T13:19:03.399628"
  },
  {
    "method_name": "Prompt_huatuo",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 5.1002655029296875,
    "editor_gpu_memory": 16044.0,
    "edit_execution_time": 9.851184844970703,
    "edit_gpu_memory": 800.0,
    "total_time": 14.95145034790039,
    "peak_gpu_memory": 17391.0,
    "rewrite_accuracy": 0.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 1.9702369689941406,
    "timestamp": "2025-07-25T13:19:36.601858"
  },
  {
    "method_name": "LoRA_llavaov",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.015743970870972,
    "editor_gpu_memory": 15504.0,
    "edit_execution_time": 54.83921575546265,
    "edit_gpu_memory": 26926.0,
    "total_time": 58.85495972633362,
    "peak_gpu_memory": 42977.0,
    "rewrite_accuracy": 0.2,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 10.967843151092529,
    "timestamp": "2025-07-25T13:20:53.432077"
  },
  {
    "method_name": "LoRA_qwen2vl",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.272334098815918,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 22.046315908432007,
    "edit_gpu_memory": 4674.0,
    "total_time": 26.318650007247925,
    "peak_gpu_memory": 21319.0,
    "rewrite_accuracy": 0.2,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 4.409263181686401,
    "timestamp": "2025-07-25T13:21:32.379978"
  },
  {
    "method_name": "LoRA_huatuo",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.3747076988220215,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 26.168321132659912,
    "edit_gpu_memory": 4590.0,
    "total_time": 30.543028831481934,
    "peak_gpu_memory": 21217.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 1.0,
    "avg_time_per_sample": 5.233664226531983,
    "timestamp": "2025-07-25T13:22:15.240438"
  },
  {
    "method_name": "WISE_llavaov",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.251491546630859,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 44.319079875946045,
    "edit_gpu_memory": 7448.0,
    "total_time": 48.570571422576904,
    "peak_gpu_memory": 23515.0,
    "rewrite_accuracy": 0.0,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 8.863815975189208,
    "timestamp": "2025-07-25T13:23:16.237170"
  },
  {
    "method_name": "WISE_qwen2vl",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.0254881381988525,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 16.767030477523804,
    "edit_gpu_memory": 2236.0,
    "total_time": 20.792518615722656,
    "peak_gpu_memory": 18881.0,
    "rewrite_accuracy": 0.4,
    "rephrase_accuracy": 0.4,
    "avg_time_per_sample": 3.3534060955047607,
    "timestamp": "2025-07-25T13:23:48.825588"
  },
  {
    "method_name": "WISE_huatuo",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.264020681381226,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 20.547228574752808,
    "edit_gpu_memory": 2246.0,
    "total_time": 24.811249256134033,
    "peak_gpu_memory": 18873.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 1.0,
    "avg_time_per_sample": 4.109445714950562,
    "timestamp": "2025-07-25T13:24:25.903682"
  },
  {
    "method_name": "GRACE_llavaov",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 3.9994735717773438,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 29.28762149810791,
    "edit_gpu_memory": 9888.0,
    "total_time": 33.287095069885254,
    "peak_gpu_memory": 25955.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 5.857524299621582,
    "timestamp": "2025-07-25T13:25:11.411094"
  },
  {
    "method_name": "GRACE_qwen2vl",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.1144797801971436,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 12.337419033050537,
    "edit_gpu_memory": 2248.0,
    "total_time": 16.45189881324768,
    "peak_gpu_memory": 18895.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 2.4674838066101072,
    "timestamp": "2025-07-25T13:25:39.424798"
  },
  {
    "method_name": "GRACE_huatuo",
    "category": "understanding",
    "num_samples": 5,
    "editor_creation_time": 4.207974195480347,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 16.31781816482544,
    "edit_gpu_memory": 2186.0,
    "total_time": 20.525792360305786,
    "peak_gpu_memory": 18815.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 3.263563632965088,
    "timestamp": "2025-07-25T13:26:11.815902"
  },
  {
    "method_name": "Prompt_llavaov",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.08773946762085,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 39.38967180252075,
    "edit_gpu_memory": 7222.0,
    "total_time": 43.4774112701416,
    "peak_gpu_memory": 23291.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 1.0,
    "avg_time_per_sample": 7.877934360504151,
    "timestamp": "2025-07-25T13:27:07.121077"
  },
  {
    "method_name": "Prompt_qwen2vl",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.085338115692139,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 15.564227819442749,
    "edit_gpu_memory": 2166.0,
    "total_time": 19.649565935134888,
    "peak_gpu_memory": 18813.0,
    "rewrite_accuracy": 0.4,
    "rephrase_accuracy": 0.4,
    "avg_time_per_sample": 3.1128455638885497,
    "timestamp": "2025-07-25T13:27:37.835462"
  },
  {
    "method_name": "Prompt_huatuo",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.167182207107544,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 19.93198251724243,
    "edit_gpu_memory": 2672.0,
    "total_time": 24.099164724349976,
    "peak_gpu_memory": 19301.0,
    "rewrite_accuracy": 0.0,
    "rephrase_accuracy": 0.2,
    "avg_time_per_sample": 3.9863965034484865,
    "timestamp": "2025-07-25T13:28:13.441436"
  },
  {
    "method_name": "LoRA_llavaov",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.001220941543579,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 95.20831751823425,
    "edit_gpu_memory": 54326.0,
    "total_time": 99.20953845977783,
    "peak_gpu_memory": 70395.0,
    "rewrite_accuracy": 0.4,
    "rephrase_accuracy": 0.4,
    "avg_time_per_sample": 19.041663503646852,
    "timestamp": "2025-07-25T13:30:04.277654"
  },
  {
    "method_name": "LoRA_qwen2vl",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.101902484893799,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 55.97649621963501,
    "edit_gpu_memory": 22398.0,
    "total_time": 60.07839870452881,
    "peak_gpu_memory": 39045.0,
    "rewrite_accuracy": 0.2,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 11.195299243927002,
    "timestamp": "2025-07-25T13:31:16.171163"
  },
  {
    "method_name": "LoRA_huatuo",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.311847448348999,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 70.65303230285645,
    "edit_gpu_memory": 21898.0,
    "total_time": 74.96487975120544,
    "peak_gpu_memory": 38527.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 1.0,
    "avg_time_per_sample": 14.13060646057129,
    "timestamp": "2025-07-25T13:32:43.181799"
  },
  {
    "method_name": "WISE_llavaov",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.205901622772217,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 97.60228848457336,
    "edit_gpu_memory": 21982.0,
    "total_time": 101.80819010734558,
    "peak_gpu_memory": 38051.0,
    "rewrite_accuracy": 0.4,
    "rephrase_accuracy": 0.4,
    "avg_time_per_sample": 19.520457696914672,
    "timestamp": "2025-07-25T13:34:37.270851"
  },
  {
    "method_name": "WISE_qwen2vl",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 3.9858856201171875,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 50.00008487701416,
    "edit_gpu_memory": 8080.0,
    "total_time": 53.98597049713135,
    "peak_gpu_memory": 24727.0,
    "rewrite_accuracy": 0.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 10.000016975402833,
    "timestamp": "2025-07-25T13:35:42.713021"
  },
  {
    "method_name": "WISE_huatuo",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.264928579330444,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 64.22075080871582,
    "edit_gpu_memory": 8060.0,
    "total_time": 68.48567938804626,
    "peak_gpu_memory": 24689.0,
    "rewrite_accuracy": 0.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 12.844150161743164,
    "timestamp": "2025-07-25T13:37:03.155181"
  },
  {
    "method_name": "GRACE_llavaov",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.040411710739136,
    "editor_gpu_memory": 15444.0,
    "edit_execution_time": 63.166014671325684,
    "edit_gpu_memory": 22394.0,
    "total_time": 67.20642638206482,
    "peak_gpu_memory": 38463.0,
    "rewrite_accuracy": 0.4,
    "rephrase_accuracy": 0.4,
    "avg_time_per_sample": 12.633202934265137,
    "timestamp": "2025-07-25T13:38:22.486744"
  },
  {
    "method_name": "GRACE_qwen2vl",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.075044393539429,
    "editor_gpu_memory": 16022.0,
    "edit_execution_time": 39.220847606658936,
    "edit_gpu_memory": 9620.0,
    "total_time": 43.295892000198364,
    "peak_gpu_memory": 26267.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 7.8441695213317875,
    "timestamp": "2025-07-25T13:39:17.251919"
  },
  {
    "method_name": "GRACE_huatuo",
    "category": "reasoning",
    "num_samples": 5,
    "editor_creation_time": 4.335139989852905,
    "editor_gpu_memory": 16004.0,
    "edit_execution_time": 53.25603175163269,
    "edit_gpu_memory": 9612.0,
    "total_time": 57.591171741485596,
    "peak_gpu_memory": 26241.0,
    "rewrite_accuracy": 1.0,
    "rephrase_accuracy": 0.0,
    "avg_time_per_sample": 10.651206350326538,
    "timestamp": "2025-07-25T13:40:26.922901"
  }
]